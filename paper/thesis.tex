\documentclass[a4paper]{report}

% do not ignore non-ascii chars
\usepackage{fontspec}

% use harvard-style referencing
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\bibliographystyle{abbrvnat}


\begin{document}

\title{IPA alignment using vector representations}
\author{Pavel Sofroniev}
\maketitle

\begin{abstract}
	This paper compares various methods of aligning IPA-encoded sound sequences using vector representations of phonemes.
\end{abstract}


\chapter{Introduction}

Most of the computational methods developed in the field of historical linguistics involve the task of aligning sound sequences,
either on its own or as a necessary step in a larger application.
In its essence, sequence alignment is a way to arrange two or more sequences together
in order to identify sub-sequences which are similar according to certain pre-defined criteria.
Usually in the context of historical linguistics these are sequences of phonological or phonetic segments comprising transcriptions of words;
and the similarity of given segments is subsequently used to infer either the cognacy of these words or rules of sound correspondences.

The standard algorithm for pairwise (i.e. aligning two sequences at a time) alignment
was originally developed by \citet{1970_Needleman_Wunsch} for aligning amino acid sequences.
Given a function that assigns scores to pairs of sequence elements,
the Needleman-Wunsch algorithm is guaranteed to find the alignment(s) that minimises/maximises the overall score.
There exist a number of variants of the basic algorithm, and many of these have been adapted for the purposes of computational linguistics;
however, the output always depends to a great extent on the scoring function.
Some of these variants have been developed for aligning more than two sequences at once,
e.g. the T-Coffee algorithm by \citet{2000_Notredame_al}, but in this paper we focus on pairwise alignment only.

Unlike in bioinformatics, which deals primarily with sequences composed out of small sets of well-defined units,
in historical linguistics encoding of sequences posed for alignment is not a solved problem.
Even though in general the standard way to encode sound sequences is IPA, the International Phonetic Alphabet,
for the purposes of alignment the overwhelming diversity of sound segments impedes the creation of a useful scoring function.
That is why most of the computational methods resort to reducing the set of sound segments in one way or another.


\chapter{Methodology}

\section{Vectors}

This section describes the different methods of obtaining phoneme vector representations that have been used in the paper.

\subsection{One-hot encoding}

Under one-hot encoding, given a vocabulary of N distinct phonemes, each phoneme would be represented as a distinct binary vector of size N,
such that exactly one of its dimensions has value 1 and all its other dimensions have value 0.
In such a vector space, all vectors have length 1 and any two non-identical vectors are orthogonal.

One-hot encoding is a simple method, both conceptually and computationally, but it cannot be very useful for producing distance measures
because under its model each phoneme is equidistant from all the others.
For the purposes of this study one-hot vector representations are used as a baseline for comparing other methods.


\subsection{PHOIBLE features}

PHOIBLE Online is an ongoing project aiming to compile a comprehensive tertiary database of the world languages' phonological inventories.
At the time of writing the database contains phonological inventories for 1672 distinct languages making use of 2160 distinct IPA segments \citep{2014_Moran_al}.

As part of the project the developers are also maintaining a table of phonological features,
effectively mapping each IPA segment encountered in the database to a unique ternary feature vector
(feature values indicate either the presence, absence, or non-applicability of the respective feature).
The feature table includes 39 distinct features and is based on research by \citet{2009_Bruce} and \citet{2011_Moisik_al}.

The PHOIBLE feature vectors comprise phoneme representations grounded in phonology and linguistics theory
that could be readily used for a variety of computational endeavours.


\subsection{phon2vec}

Word2vec comprises two closely related model architectures for computing vector representations of words, first introduced by \citet{2013_Mikolov_al}.
As this study concerns phonemes rather than words, we call the method phon2vec.

Unlike the vectors obtained from one-hot encodings or PHOIBLE's feature matrix, phon2vec vectors are the output of the model being trained on data.
The training data that we use is the set of all transcriptions from the NorthEuraLex,
a comprehensive lexicostatistical database that provides IPA-encoded lexical data for languages of, primarily but not exclusively, Northern Eurasia \citep{2017_Dellert_JÃ¤ger}.
At the time of writing the database covers 1016 concepts from 107 languages, resulting in 121614 IPA transcriptions.
The latter are tokenised using ipatok, which is covered somewhere else.

The word2vec/phon2vec model is a parametric one, and the resulting vector representations can vary widely depending on the parameters' values.


\subsection{Neural network embeddings}




\chapter{Evaluation}


\chapter{Conclusion}


\bibliography{references}


\end{document}
