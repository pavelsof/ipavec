\documentclass[a4paper]{report}

% do not ignore non-ascii chars
\usepackage{fontspec}

% use harvard-style referencing
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\bibliographystyle{abbrvnat}

% needed for URLs
\usepackage[hidelinks]{hyperref}

% beautiful tables
\usepackage{booktabs}


\begin{document}

\title{IPA alignment using vector representations}
\author{Pavel Sofroniev, Çağri Çöltekin (advisor)}
\date{February 2018}
\maketitle

\begin{abstract}
	This paper compares various methods of aligning IPA-encoded sound sequences using vector representations of phonemes.
\end{abstract}


\chapter{Introduction}

Most of the computational methods developed in the field of historical linguistics involve the task of aligning sound sequences,
either on its own or as a necessary step in a larger application.
In its essence, sequence alignment is a way to arrange two or more sequences together
in order to identify sub-sequences which are similar according to certain pre-defined criteria.
Usually in the context of historical linguistics these are sequences of phonological or phonetic segments comprising transcriptions of words;
and the similarity of given segments is subsequently used to infer either the cognacy of these words or rules of sound correspondences.

The standard algorithm for pairwise (i.e. aligning two sequences at a time) alignment
was originally developed by \citet{1970_Needleman_Wunsch} for aligning amino acid sequences.
Given a function that assigns scores to pairs of sequence elements,
the Needleman-Wunsch algorithm is guaranteed to find the alignment(s) that minimises/maximises the overall score.
There exist a number of variants of the basic algorithm, e.g. \citet{1981_Smith_Waterman},
and many of these have been adapted for the purposes of computational linguistics;
however, the output always depends to a great extent on the scoring function.
Some of these variants have been developed for aligning more than two sequences at once,
e.g. the T-Coffee algorithm by \citet{2000_Notredame_al}, but in this paper we focus on pairwise alignment only.

Unlike in bioinformatics, which deals primarily with sequences composed out of small sets of well-defined units,
in historical linguistics the encoding of sequences posed for alignment is not a solved problem.
Even though in general the standard way to encode sound sequences is IPA, the International Phonetic Alphabet,
for the purposes of alignment the overwhelming diversity of sound segments impedes the creation of a useful scoring function.
That is why modern sound sequence alignment methods tend to operate on small sets of segments;
IPA-encoded data is handled by translating it into the respective reduced alphabet.
While this constitutes a valid approach to tackle the problem of deriving a scoring function for the rich set of IPA segments,
in this paper we experiment with an alternative approach: vectorisation.
The goal is to obtain representations of IPA segments in a vector space
in which the cosine similarity between segments' vectors would reflect the segments' similarity and, hence, probability of shared ancestry.

Our inspiration for experimenting with vector representations (also known as embeddings)
is drawn from the success of such methods in other fields of computational linguistics:
e.g. \citet{2013_Mikolov_al} affirm that vector representations of words exhibit useful semantic and syntactic properties;
and \citet{2014_Chen_Manning} use word embeddings for transition-based parsing.
More closely related to our topic,
\citet{2018_Silfverberg_al} obtain vector representations of orthographic segments for Finnish, Spanish and Turkish
from recurrent neural networks trained to perform an inflection task.


\section{Background}

This section provides a concise overview of sound sequence alignment methods reported in the computational historical linguistics literature.


\subsection{Early methods}

An early method for sound sequence alignment reported in the literature is the one proposed by \citet{1996_Covington},
which was subsequently improved and extended in \citet{1998_Covington}.
Although the method does not take advantage of any standard sequence alignment algorithm and instead performs branch-and-bound search,
it does use a scoring function.
This function assigns hand-crafted values depending on whether the segments are consonants, vowels, or glides.
Due to this simple classification, the method could be adapted to work with IPA sequences (the author uses an undocumented encoding);
however, the better performance of more complex class-based methods have shown that such coarse three-way classification is not sufficient.

ALINE, first introduced by \citet{2000_Kondrak} and further elaborated in \citet{2002_Kondrak_Hirst} and \citet{2003_Kondrak},
employs a complex scoring function that is built around multi-valued (e.g. manner of articulation) and binary (e.g. nasality) phonetic features,
each with its relative weight, as well as some additional parameters (e.g. relative weight of vowels).
While some of these numerical values have been determined based on observations from the field of articulatory phonetics,
others have been established by trial and error.


\subsection{ASJP}

The Automated Similarity Judgement Program (ASJP) is an actively developed database aiming to provide
the translations of a set of 40 basic concepts into all the world's languages.
At the time of writing the database covers 294\,548 words from 7221 languages \citep{2016_Wichmann_al}.
The words are encoded using a uniform transcription consisting of 34 consonants and 7 vowels,
originally introduced by \citet{2008_Brown_al} as ASJPcode but now commonly known as ASJP.
This restricted set of encoding symbols results in much of the phonetic information being lost,
but the authors argue that such information might not be that valuable for certain applications.
The simplified encoding also allows for a greater reach of the database as many of the world's languages lack sufficiently detailed phonetic record.

\citet{2013_Jäger} uses the ASJP database to calculate the partial mutual information (PMI) scores\,---\,a logarithmic measure
for sound similarity based on occurrence and co-occurrence of the sound segments in data\,---\,for each pair of ASJP segments.
These PMI scores are successfully employed for inferring phylogenetic trees through determining language distances from PMI-based sequence alignment.
Unlike the aforementioned methods, this one obtains its scoring function in a data-driven manner, an approach also embraced in our paper.

PMI-based scoring has been used on IPA-encoded datasets as well, by converting the sequences to ASJP as a pre-processing step.
As an example, \citet{2016_Jäger_Sofroniev} use this approach to train a support vector machine (SVM) for cognate classification.


\subsection{Other methods}

The sound-class-based phonetic alignment (SCA) method developed by \citet{2012_List} employs a set of 28 sound classes.
It operates on IPA sequences by converting the segments into their respective sound classes, aligning the sound class tokens, and then converting these back into IPA.
The scoring function is hand-crafted to reflect the perceived probabilities of sound change transforming a segment of one class into a segment of another.


\chapter{Methodology}

Our source code is licensed under GNU GPLv3 and publicly available at TBA.


\section{Vectors}

This section describes the different methods of obtaining vector representations that we have experimented with.

\subsection{One-hot encoding}

Under one-hot encoding, given a vocabulary of $N$ distinct segments, each segment would be represented as a distinct binary vector of size $N$,
such that exactly one of its dimensions has value 1 and all its other dimensions have value 0.
In such a vector space, all vectors have length 1 and any two non-identical vectors are orthogonal.

One-hot encoding is a simple method, both conceptually and computationally, but it cannot be very useful for producing distance measures
because under its model each segment is equidistant from all the others.
For the purposes of this study one-hot vector representations are used as a baseline for comparing other methods.


\subsection{PHOIBLE features}

PHOIBLE Online is an ongoing project aiming to compile a comprehensive tertiary database of the world languages' phonological inventories.
At the time of writing the database contains phonological inventories for 1672 distinct languages making use of 2160 distinct IPA segments \citep{2014_Moran_al}.

As part of the project the developers are also maintaining a table of phonological features,
effectively mapping each IPA segment encountered in the database to a unique ternary feature vector
(feature values indicate either the presence, absence, or non-applicability of the respective feature).
The feature table includes 39 distinct features and is based on research by \citet{2009_Bruce} and \citet{2011_Moisik_al}.

The PHOIBLE feature vectors comprise IPA segment representations grounded in phonology and linguistics theory
that could be readily used for a variety of computational endeavours.


\subsection{phon2vec}

Word2vec comprises two closely related model architectures for computing vector representations of words, first introduced by \citet{2013_Mikolov_al}.
As this study concerns IPA segments rather than words, we call the method phon2vec.

Unlike the vectors obtained from one-hot encodings or PHOIBLE's feature matrix, phon2vec vectors are the output of the model being trained on data.

The word2vec/phon2vec model is a parametric one, and the resulting vector representations can vary widely depending on the parameters' values.


\subsection{NN embeddings}

\subsection{RNN embeddings}




\chapter{Experiments}

In order to evaluate the performance of the methods put forward in the last chapter,
we use the Benchmark Database for Phonetic Alignments (BDPA) compiled by \citet{2014_List_Prokić}.
The database contains 7126 aligned pairs of IPA sequences collected from 12 source datasets,
covering languages from the Aymaran, Indo-European, Japonic, Quechuan, Sino-Tibetan, and Uralic language families.
The database also features the small set of 82 selected pairs used by \citet{1996_Covington} to evaluate his method, encoded in IPA.

Our methods are also compared against SCA, the current state-of-the-art method for aligning IPA sequences developed by \citet{2012_List},
which is briefly described in the introductory chapter.

In order to quantify the methods' performance, we employ an intuitive evaluation scheme which is also used in \citet{2002_Kondrak_Hirst}:
each correct alignment yields 1 point;
if $n$ alternative alignments are produced for a particular pair and the correct one is among these the score is $\frac{1}{n}$ points;
partially correct alignment do not score points.
The percentage scores are obtained by dividing the points by the total number of pairs.
Pairs for which BDPA includes more than one correct alignment are excluded.


\section{Training data}

Our training data is sourced from NorthEuraLex, a comprehensive lexicostatistical database
that provides IPA-encoded lexical data for languages of, primarily but not exclusively, Northern Eurasia \citep{2017_Dellert_Jäger}.
At the time of writing the database covers 1016 concepts from 107 languages, resulting in 121\,614 IPA transcriptions.
The latter are tokenised using ipatok, an open source Python package for tokenising IPA strings developed by us.\footnote{\url{https://pypi.python.org/pypi/ipatok}}
Phon2vec and NN are trained on the set of all tokenised transcriptions.
As NorthEuraLex does not include cognacy information, RNN is trained on the set of tokenised transcriptions of the word pairs constituting probable cognates\,---\,pairs
in which the words belong to different languages, are linked to the same concept, and have normalised Levenshtein distance lower than 0,5.
Admittedly, this threshold value is chosen somewhat arbitrarily;
we have also experimented with thresholds of 0,4 and 0,6, but setting the cutoff at 0,5 yields better-performing embeddings.


\section{Results}

The percentage-score evaluation of the output of running our proposed methods and SCA on the BDPA datasets is summarised in Table~\ref{tab:results}.

\begin{table}[h]
	\centering\small
	\begin{tabular}{l *{6}{c}}
		\toprule
		& one-hot & phoible & phon2vec & nn & nn\,+\,rnn & sca \\
		\midrule
		Andean		&	86,47 &	87,93 &	97,52 &	99,32 &	99,49 & 99,66 \\
		Bai			&	51,89 &	62,21 &	60,90 &	74,34 &	75,15 & 77,64 \\
		Bulgarian	&	60,54 &	80,54 &	77,98 &	82,55 &	86,70 & 89,34 \\
		Dutch		&	14,16 &	25,65 &	26,00 &	32,50 &	32,50 & 42,20 \\
		French		&	42,94 &	62,92 &	68,94 &	74,30 &	77,04 & 80,90 \\
		Germanic	&	39,82 &	51,81 &	54.51 &	71,78 &	72,50 & 83,45 \\
		Japanese	&	53.56 &	65.04 &	73.74 &	62,71 &	71,08 & 82,19 \\
		Norwegian	&	59.24 &	78.74 &	73.54 &	83,43 &	88,99 & 91,72 \\
		Ob-Ugrian	&	59.58 &	77.87 &	73.35 &	78,04 &	82,55 & 86,04 \\
		Romance		&	40.48 &	71.28 &	63.16 &	76,37 &	77,55 & 95,62 \\
		Sinitic		&	27,06 &	28,02 &	30,42 &	70,93 &	72,59 & 61,11 \\
		Slavic		&	76.96 &	90.73 &	84.22 &	89,89 &	96,81 & 94,15 \\
		\addlinespace
		Global		&	44,76 &	58,28 &	56,32 &	63,89 &	78,45 & 83,12 \\
		Covington	&	60,61 &	82,42 &	80,18 &	82,52 &	82,52 & 90,24 \\
		\bottomrule
	\end{tabular}
	\caption{Scores, as percentage of total alignment pairs}
	\label{tab:results}
\end{table}


\section{Discussion}


\chapter{Conclusion}


\bibliography{references}


\end{document}
